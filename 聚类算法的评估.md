# 聚类算法的评估

数据的聚类依赖于实际需求， 同时也依赖于**数据的特征度量**以及**评估数据相似性**的方法。相比于监督学习， 非监督学习通常没有标注数据，** 模型、 算法的设计**直接影响最终的输出和模型的性能。 为了评估不同聚类算法的性能优劣， 我们需要了解常见的数据簇的特点。
 * **以中心定义的数据簇：**这类数据集合倾向于球形分布， 通常中心被定义为质
心， 即此数据簇中所有点的平均值。 集合中的数据到中心的距离相比到其他
簇中心的距离更近。
* **以密度定义的数据簇：** 这类数据集合呈现和周围数据簇明显不同的密度， 或
稠密或稀疏。 当数据簇不规则或互相盘绕， 并且有噪声和离群点时， 常常使
用基于密度的簇定义。
* **以连通定义的数据簇：** 这类数据集合中的数据点和数据点之间有连接关系，
整个数据簇表现为图结构。 该定义对不规则形状或者缠绕的数据簇有效。
* **以概念定义的数据簇：** 这类数据集合中的所有数据点具有某种共同性质。

聚类评估的任务**是估计在数据集上进行聚类的可行性**， **以及聚类方法产生结果的质量**。 分为以下三个子任务：

1. **估计聚类趋势**。这一步骤是检测数据分布中是否存在非随机的簇结构。**如果数据是基本随机的， 那么聚类的结果也是毫无意义的。**

   * **可以观察聚类误差是否随聚类类别数量的增加而单调变化**，如果数据是基本随机的，那么聚类误差随聚类类别数量的增加而变化的幅度就不显著，找不到一个合适的K对应数据的真是簇数。

   * 我们也可以应用霍普金斯统计量（Hopkins Statistic） 来判断数据在空间上的随机性。 步骤如下：
     首先，从所有样本中随机找n个点，然后为每一个点在样本空间中找到一个离他最近的点，并计算它们之间的聚类$x_i$,从而得到距离向量$x_1,x_2,...,x_n$;
     然后，从样本的可能取值范围内随机生成n个点，对每一个随机生产的点，找到一个离它最近的样本点，并计算它们之间的距离，得到$y_1,y_2,...,y_n$。霍普金斯统计量H可以表示为：
     <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145022014-940308927.png" style="zoom:50%;" />，

     如果样本接近随机分布， 那么<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145046854-1876958045.png" style="zoom:50%;" />和<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145100117-1494669570.png" style="zoom:50%;" />的取值应该比较接近， 即H的值接近于0.5； 如果聚类趋势明显， 则随机生成的样本点距离应该远大于实际样本点的距离， 即<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145121534-669739535.png" style="zoom:50%;" />， H的值接近于1。

2. **判定数据簇数**。确定聚类趋势之后， 我们需要找到与真实数据分布最为吻合的簇数， 据此判定聚类结果的质量。 例如用手肘法和Gap Statistic方法。 需要说明的是， **用于评估的最佳数据簇数可能与程序输出的簇数是不同的。**
3. **测定聚类质量。**在无监督的情况下， 我们可以通过考察**簇间的分离情况**和**簇内的紧凑情况**来评估聚类的效果。 
   下面为几种常用的指标：
   * **轮廓系数**： 给定一个点p， 该点的轮廓系数定义为
     <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145356739-11391808.png" style="zoom:50%;" />
     其中a(p)是点p与同一簇中的其他点p’之间的平均距离； b(p)是点p与另一个不同簇中的点之间的最小平均距离（如果有n个其他簇， 则只计算和点p最接近的一簇中的点与该点的平均距离） 。 **a(p)反映的是p所属簇中数据的紧凑程度**， **b(p)反映的是该簇与其他临近簇的分离程度**。 **显然， b(p)越大， a(p)越小， 对应的聚类质量越好**， 因此我们将所有点对应的轮廓系数s(p)求平均值来度量聚类结果的质量。即s(p)越接近于1，聚类效果越好。
   * **均方根标准偏差**（Root-mean-square standard deviation， RMSSTD） ： 用来衡量聚结果的同质性， 即紧凑程度， 定义为
     <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145553970-642764626.png" style="zoom:50%;" />
     其中$C_i$代表第i个簇， $c_i$是该簇的中心， x∈ $C_i$代表属于第i个簇的一个样本点， $n_i$为第i个簇的样本数量， P为样本点对应的向量维数。 可以看出， 分母对点的维度P做了惩罚， 维度越高， 则整体的平方距离度量值越大。<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145919518-1438831214.png" style="zoom:50%;" />， 其中n为样本点的总数， NC为聚类簇的个数， 通常NC<<n， 因此<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145942615-840354293.png" style="zoom:50%;" />的值接近点的总数， 为一个常数。 综上， **RMSSTD可以看作是经过归一化的标准差**。该值越小越好。
   * R方（R-Square） ： 可以用来衡量聚类的差异度，定义为：
     <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223145835576-1348745273.png" style="zoom:50%;" />，
     其中D代表整个数据集， c代表数据集D的中心点， 从而 <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150014847-1032272926.png" style="zoom: 50%;" />代表将数据集D看作单一簇时的平方误差和。 与上一指标RMSSTD中的定义相同，<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150039454-2122393952.png" style="zoom:50%;" />代表将数据集聚类之后的平方误差和， **所以RS代表了聚类之后的结果与聚类之前相比， 对应的平方误差和指标的改进幅度。该幅度越大越好。**
   * **改进的HubertΓ统计**： 通过数据对的不一致性来评估聚类的差异， 定义为
     <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150212839-44623483.png" style="zoom:50%;" />
     其中d(x,y) 表示点x到点y之间的距离，<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150813272-1480166389.png" style="zoom:50%;" />代表点x所在的簇中心$c_i$与点y所在的簇中心$c_j$之间的距离，<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150832569-1729905699.png" style="zoom:50%;" />为所有(x,y)点对的个数， 因此指标相当于对每个点对的和做了归一化处理。 **理想情况下， 对于每个点对(x,y)， 如果d(x,y)越小，对应的 <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150813272-1480166389.png" style="zoom:50%;" />也应该越小（ 特别地， 当它们属于同一个聚类簇时，<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150813272-1480166389.png" style="zoom:50%;" />=0） ； 当d(x,y)越大时，<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200223150813272-1480166389.png" style="zoom:50%;" /> 的取值也应当越大， 所以Γ值越大说明聚类的结果与样本的原始距离越吻合， 也就是聚类质量越高。**