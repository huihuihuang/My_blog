>本博客根据 百面机器学习,算法工程师带你去面试 一书总结归纳,公式都是出自该书.
>本博客仅为个人总结学习,非商业用途,侵删.
>网址 http://www.ptpress.com.cn

### 目录：

* PCA最大方差理论

* PCA最小平方误差理论 



在机器学习中， 数据通常需要被表示成向量形式以输入模型进行训练。 但是在对向维向量进行处理和分析时， 会极大地消耗系统资源， 甚至产生维度灾难。 因此， 对特征向量进行降维， 即用一个低维度的向量表示原始高维度的特征就显得尤为重要。 

PCA（Principal Components Analysis），它属于一种线性，非监督，全局的降维方法。旨在找到数据中的主成分，并利用这些主成分表征原始数据，从而达到降维的目的。

## PCA最大方差理论

我们不妨先从最简单的二维数据进行推到，看PCA是如何工作的。如下图所示：
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218115604364-1679880762.png" alt="" style="zoom:67%;" />
上图中表示经过中心化的一组数据,我们很容易看出主成分所在的轴的大致方向,即图b中黄线所在的轴。因为在黄线所处的轴上,数据分布的更为分散,意味着数据在这个方向上方差更大。所以从上图可以引出,**PCA的目标即最大化投影方差**,也就是让数据在主轴上投影的方差最大。

对给定的一组数据点{$v_1,v_2,…,v_n$},其中所以向量均为列向量,中心化后的表示为：
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218115749987-1709663442.png" alt="" style="zoom:50%;" />=<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218115808832-1012906064.png" alt="" style="zoom:50%;" />,其中<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218115843946-719980978.png" alt="" style="zoom: 50%;" />

进行中心化的意义是让数据投影后的平均值为0，更容易描述找到的投影向量。且在进行了中心化后,所有样本投影之后的均值为0，即<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218123214566-2015077988.png" style="zoom:45%;" />

我们知道向量内积在几何上表示为第一个向量投影到第二个向量上的长度,因此向量$x_i$在ω（ 单位方向向量） 上的投影坐标可以表示为 <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218122250917-852453712.png" style="zoom:50%;" />。  所以目标是找到一个投影方向ω， 使得$x_1,x_2,...,x_n$,在ω上的投影方差尽可能大。  

所以投影后得方差可以表示为:
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218122420039-1549852282.png" style = "zoom:50%;"/>
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218122538052-252203996.png" style = "zoom:50%;"/>
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218122600513-324755458.png" style = "zoom:45%;"/>
仔细观察可以发现，<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218122717327-1991763540.png" style = "zoom:40%;"/>其实就是样本的**协方差矩阵**，我们将其写作Σ。由于ω是单位方向向量， 即有$ω^Tω$=1。因此我们要求解的最大化问题，可以表示为：
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218122801660-802411150.png" style = "zoom:50%;"/>

引入拉格朗日乘子， 并对ω求导令其等于0， 便可以推出Σω=λω， 此时
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218122855224-1300787182.png" style = "zoom:50%;"/>

根据我们学过的线性代数知识，我们会发现，原来,$x$投影后的方差就是协方差矩阵的特征值。**我们要找的最大的方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应得特征向量**。次投影方向位于最佳投影方向得正交空间中，是第二大特征向量对应得特征向量，以此类推。
所以我们可以得到PCA求解得步骤：

* 1,对样本数据进行中心化;
* 2求样本得协方差矩阵;
* 3,对协方差矩阵进行特征值分解,将特征值从大到小排序;
* 4,去特征值前d大对应得特征向量$ω_1,ω_2,...,ω_d$,通过以下映射将n维样本映射到d维。
  <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218123805185-2038682671.png" style = "zoom:50%;"/>

新的$x_i^′$的第d维就是$x_i$在第d个主成分$ω_d$方向上的投影， 通过选取最大的d个特征值对应的特征向量， 我们将方差较小的特征（ 噪声） 抛弃， 使得每个n维列向量$x_i$被映射为d维列向量$x_i^′$， 定义降维后的信息占比为
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218124902692-569737052.png" style = "zoom:50%;"/>



**总结:** PCA是一种线性降维方法,有一定的局限性,我们可以通过核映射对PCA进行扩展得到核主成分分析(KPCA),也可以通过流形映射的降维方法， 比如等距映射、 局部线性嵌入、 拉普拉斯特征映射等， 对一些PCA效果不好的复杂数据集进行非线性降维操作。


## PCA最小平方误差理论
前面求解一条直线使得样本点投影到该直线上的方差最大。从求解直线的思路出发,很容易联想到数学中的线性回归问题,其目标也是求解一个线性函数使得对应直线能够很好地拟合样本点集合。从这个角度出发,**PCA的目标就会转换为一个回归问题,在高维空间中,我们实际要找一个d维超平面,使得数据点到这个超平面的距离平方和最小.**。

以d=1为例,超平面退化为直线,即把样本投影到最佳直线,最小化的就是所有点到直线的距离平方之和。如下图所示:

<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218140941901-1717938008.png" style="zoom:50%;" />

数据集中每个点$x_k$到d维超平面D的距离为:
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218141131064-1210820617.png" alt="image-20200218141137330" style="zoom:50%;" />

其中<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143046247-879132641.png" style="zoom:45%;"/> 表示$x_k$在超平面D上的投影向量。如果该超平面由d个标准正交基<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143221571-838304374.png" style="zoom:50%;" />构成,根据线性代数理论<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143046247-879132641.png" style="zoom:50%;" />可以由这组基线性表示:
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143314886-1602035720.png" style="zoom:50%;" />
其中$ω_i^T x_k$表示$x_k$在$ω_i$方向上投影的长度.因此,<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143046247-879132641.png" style="zoom:50%;" />实际上就是$x_k$在**W**这组标准正交基下的坐标.而PCA要优化的目标为:
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143528717-1423049197.png" style="zoom:50%;" />

由向量内积的性质,我们可以知道,<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143630684-713890229.png" style="zoom:50%;" />,于是可以将上面式子中的每一个距离展开

<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143703446-78004248.png" style="zoom:50%;" />

根据展开后的式子来看,

* 第一项$x_k^Tx_k$与选取的**W**无关,是个常数。
* 第二项为:
  <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143848655-707988429.png" style="zoom:50%;" />
  <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143852959-1614967819.png" style="zoom:50%;" />
  <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143900116-1264418455.png" style="zoom:50%;" />
* 第三项为:
  <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143915598-1752516742.png" style="zoom:50%;" />
  <img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218143918422-1865057247.png" style="zoom:50%;" />

第三项中,$ω_i^T x_k$ 和$ω_j^T x_k$ 表示投影长度， 都是数字.且当i≠j时,$ω_i^Tω_j=0$,因此第三项的交叉项中只剩下d项:
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144325632-708790911.png" style="zoom:50%;" />

我们可以看到<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144408610-1080840566.png" style="zoom:50%;" />实际上就是矩阵$W^Tx_kx_k^TW$的迹(对角线之和),于是可以将距离差的平方继续化简为:
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144611838-1160557758.png" style="zoom:50%;" />
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144624186-1656309244.png" style="zoom:50%;" />

所以原来的优化目标函数可以写成:
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144643362-576948197.png" style="zoom:50%;" />
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144650655-2127149169.png" style="zoom:50%;" />
根据矩阵乘法的性质<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144717105-163322314.png" style="zoom:50%;" />,因此优化问题可以转化为<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144739275-907968645.png" style="zoom:50%;" />,这等价于求解带约束的优化问题
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144804397-2085440238.png" style="zoom:50%;" />
如果我们对**W**中的d个基$ω_1,ω_2,...,ω_d$依次求解， 就会发现和最大方差理论的方法
完全等价。 比如当d=1时， 我们实际求解的问题是
<img src="https://img2018.cnblogs.com/blog/1873709/202002/1873709-20200218144951978-686611947.png" style="zoom:50%;" />
最佳直线ω与最大方差法求解的最佳投影方向一致， 即协方差矩阵的最大特征值所对应的特征向量， 差别仅是协方差矩阵Σ的一个倍数， 以及常数 偏差，但这并不影响我们对最大值的优化。

**总结:**我们从最小平方误差的角度解释了PCA的原理、 目标函数和求解方法。 不难发现，这与最大方差角度殊途同归， 从不同的目标函数出发， 得到了相同的求解方法。 